{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45a22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976a8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"TImages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6536a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (244, 244)\n",
    "classes = [\"0\", \"1\", \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c72303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance with data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39319dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in classes:\n",
    "        path = os.path.join(directory, category)\n",
    "        class_num = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))\n",
    "                new_array = cv2.resize(img_array, image_size)\n",
    "\n",
    "                # Generate and store augmented images\n",
    "                augmented_images = []\n",
    "                augmented_images.append(new_array)  # Original image\n",
    "                img_array_aug = new_array.reshape((1,) + new_array.shape)\n",
    "                i = 0\n",
    "                for batch in datagen.flow(img_array_aug, batch_size=1):\n",
    "                    augmented_images.append(batch[0])\n",
    "                    i += 1\n",
    "                    if i >= 5:  # Generate 5 augmented images per input image\n",
    "                        break\n",
    "\n",
    "                for augmented_image in augmented_images:\n",
    "                    image_hsv = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2HSV)\n",
    "                    training_data.append([image_hsv, class_num])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d4e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6011bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5850\n"
     ]
    }
   ],
   "source": [
    "lenofimage = len(training_data)\n",
    "print(lenofimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0e3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for categories, label in training_data:\n",
    "    X.append(categories)\n",
    "    y.append(label)\n",
    "X= np.array(X).reshape(lenofimage,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbf4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8b1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac041ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GB MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265bf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1a7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6554a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {'n_estimators': [50, 100],\n",
    "              'learning_rate': [0.01, 0.1],\n",
    "              'max_depth': [3, 5],\n",
    "              'max_features': ['auto', 'log2'],\n",
    "              'loss': ['log_loss'],\n",
    "              'subsample': [0.5, 1],\n",
    "             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db3b7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch object without cross-validation\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=5, verbose=2)  # Set cv=None for no cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "926c776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   1.0s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.2s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.2s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.2s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.2s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.01, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=50, subsample=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=50, subsample=1; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=3, max_features=log2, n_estimators=100, subsample=1; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=50, subsample=1; total time=   0.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=auto, n_estimators=100, subsample=1; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   5.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=50, subsample=1; total time=   7.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   8.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=1; total time=  13.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\tasli\\anaconda3\\envs\\coad\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.93568376 0.93568376\n",
      " 0.93696581 0.93760684        nan        nan        nan        nan\n",
      " 0.93589744 0.93568376 0.93846154 0.94145299        nan        nan\n",
      "        nan        nan 0.95235043 0.95876068 0.95769231 0.95961538\n",
      "        nan        nan        nan        nan 0.95534188 0.96068376\n",
      " 0.96047009 0.96239316]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1], &#x27;loss&#x27;: [&#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 5], &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100], &#x27;subsample&#x27;: [0.5, 1]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1], &#x27;loss&#x27;: [&#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 5], &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100], &#x27;subsample&#x27;: [0.5, 1]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.01, 0.1], 'loss': ['log_loss'],\n",
       "                         'max_depth': [3, 5], 'max_features': ['auto', 'log2'],\n",
       "                         'n_estimators': [50, 100], 'subsample': [0.5, 1]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the GridSearch object to your data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f655292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Gaussian Naive Bayes Parameters: {'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 1}\n",
      "Accuracy: 95.13%\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimator and evaluate it\n",
    "best_nb = grid_search.best_estimator_\n",
    "y_test_pred = best_nb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best Gaussian Naive Bayes Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b819f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work progressing...........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e85d5980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.92885523,  0.82562628,  0.76280761,  0.77501326,  3.46935534,\n",
       "         5.14681644,  5.40348649,  8.85015883,  0.73314538,  0.74394655,\n",
       "         0.73154397,  0.71794233,  4.58670306,  7.44206715,  7.82554197,\n",
       "        13.42605267,  0.72374225,  0.71313982,  0.7909555 ,  0.72234197,\n",
       "         3.50089011,  5.23683195,  5.43226509,  8.93676095,  0.76074944,\n",
       "         0.74394569,  0.72574301,  0.74914722,  4.63431287,  7.47467346,\n",
       "         7.75472832, 13.43844905]),\n",
       " 'std_fit_time': array([0.18007436, 0.04238815, 0.04296641, 0.08632073, 0.01992155,\n",
       "        0.02965092, 0.01765663, 0.04802429, 0.04456973, 0.03653884,\n",
       "        0.08522727, 0.04284418, 0.0281799 , 0.03730984, 0.01587743,\n",
       "        0.07018675, 0.04665927, 0.04584724, 0.06519967, 0.04355802,\n",
       "        0.03038746, 0.01867365, 0.03062091, 0.04783932, 0.09574472,\n",
       "        0.07967842, 0.04997513, 0.02636425, 0.02801697, 0.04680876,\n",
       "        0.01913992, 0.07719319]),\n",
       " 'mean_score_time': array([0.        , 0.        , 0.        , 0.        , 0.38710647,\n",
       "        0.38670702, 0.39710932, 0.39550238, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.39347816, 0.39347796, 0.41268148,\n",
       "        0.40813131, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39047699, 0.38787918, 0.39628086, 0.39847851, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.39127769, 0.39307747,\n",
       "        0.40687966, 0.40487895]),\n",
       " 'std_score_time': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.89866172e-03, 1.02042260e-03, 1.41461682e-03, 4.95608962e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.20035856e-03, 1.02009474e-03, 3.93048002e-03, 1.34497666e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.72069909e-03, 1.71940655e-03, 2.31062226e-03, 4.80050842e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.16558302e-03, 1.68317438e-06, 5.26863663e-03, 1.32656190e-03]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_loss': masked_array(data=['log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['auto', 'auto', 'auto', 'auto', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'auto', 'auto', 'auto', 'auto', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'auto', 'auto', 'auto', 'auto', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'auto', 'auto', 'auto', 'auto',\n",
       "                    'log2', 'log2', 'log2', 'log2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 50, 100, 100, 50, 50, 100, 100, 50, 50, 100, 100,\n",
       "                    50, 50, 100, 100, 50, 50, 100, 100, 50, 50, 100, 100,\n",
       "                    50, 50, 100, 100, 50, 50, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1,\n",
       "                    0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1,\n",
       "                    0.5, 1, 0.5, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 50,\n",
       "   'subsample': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1}],\n",
       " 'split0_test_score': array([       nan,        nan,        nan,        nan, 0.93589744,\n",
       "        0.93589744, 0.93803419, 0.93910256,        nan,        nan,\n",
       "               nan,        nan, 0.93696581, 0.93589744, 0.94017094,\n",
       "        0.94337607,        nan,        nan,        nan,        nan,\n",
       "        0.94871795, 0.96047009, 0.95512821, 0.96047009,        nan,\n",
       "               nan,        nan,        nan, 0.95299145, 0.95833333,\n",
       "        0.95940171, 0.96047009]),\n",
       " 'split1_test_score': array([       nan,        nan,        nan,        nan, 0.93589744,\n",
       "        0.93589744, 0.93696581, 0.93696581,        nan,        nan,\n",
       "               nan,        nan, 0.93589744, 0.93589744, 0.94017094,\n",
       "        0.94230769,        nan,        nan,        nan,        nan,\n",
       "        0.95299145, 0.95940171, 0.95833333, 0.96047009,        nan,\n",
       "               nan,        nan,        nan, 0.95619658, 0.96153846,\n",
       "        0.96153846, 0.96260684]),\n",
       " 'split2_test_score': array([       nan,        nan,        nan,        nan, 0.93589744,\n",
       "        0.93589744, 0.93696581, 0.93696581,        nan,        nan,\n",
       "               nan,        nan, 0.93589744, 0.93589744, 0.93696581,\n",
       "        0.93696581,        nan,        nan,        nan,        nan,\n",
       "        0.94978632, 0.95619658, 0.95833333, 0.95940171,        nan,\n",
       "               nan,        nan,        nan, 0.95512821, 0.95940171,\n",
       "        0.95726496, 0.96260684]),\n",
       " 'split3_test_score': array([       nan,        nan,        nan,        nan, 0.93589744,\n",
       "        0.93589744, 0.93803419, 0.93910256,        nan,        nan,\n",
       "               nan,        nan, 0.93589744, 0.93589744, 0.93803419,\n",
       "        0.94230769,        nan,        nan,        nan,        nan,\n",
       "        0.95619658, 0.96153846, 0.95726496, 0.96047009,        nan,\n",
       "               nan,        nan,        nan, 0.95512821, 0.96474359,\n",
       "        0.96260684, 0.96474359]),\n",
       " 'split4_test_score': array([       nan,        nan,        nan,        nan, 0.93482906,\n",
       "        0.93482906, 0.93482906, 0.93589744,        nan,        nan,\n",
       "               nan,        nan, 0.93482906, 0.93482906, 0.93696581,\n",
       "        0.94230769,        nan,        nan,        nan,        nan,\n",
       "        0.95405983, 0.95619658, 0.95940171, 0.95726496,        nan,\n",
       "               nan,        nan,        nan, 0.95726496, 0.95940171,\n",
       "        0.96153846, 0.96153846]),\n",
       " 'mean_test_score': array([       nan,        nan,        nan,        nan, 0.93568376,\n",
       "        0.93568376, 0.93696581, 0.93760684,        nan,        nan,\n",
       "               nan,        nan, 0.93589744, 0.93568376, 0.93846154,\n",
       "        0.94145299,        nan,        nan,        nan,        nan,\n",
       "        0.95235043, 0.95876068, 0.95769231, 0.95961538,        nan,\n",
       "               nan,        nan,        nan, 0.95534188, 0.96068376,\n",
       "        0.96047009, 0.96239316]),\n",
       " 'std_test_score': array([       nan,        nan,        nan,        nan, 0.00042735,\n",
       "        0.00042735, 0.00117035, 0.00128205,        nan,        nan,\n",
       "               nan,        nan, 0.0006757 , 0.00042735, 0.00144922,\n",
       "        0.00228143,        nan,        nan,        nan,        nan,\n",
       "        0.00275301, 0.00219992, 0.00144922, 0.00124593,        nan,\n",
       "               nan,        nan,        nan, 0.00141736, 0.00228143,\n",
       "        0.00191117, 0.00141736]),\n",
       " 'rank_test_score': array([17, 17, 17, 17, 14, 14, 12, 11, 17, 17, 17, 17, 13, 14, 10,  9, 17,\n",
       "        17, 17, 17,  8,  5,  6,  4, 17, 17, 17, 17,  7,  2,  3,  1])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e37c36c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928855</td>\n",
       "      <td>0.180074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.825626</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.762808</td>\n",
       "      <td>0.042966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.775013</td>\n",
       "      <td>0.086321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.469355</td>\n",
       "      <td>0.019922</td>\n",
       "      <td>0.387106</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.146816</td>\n",
       "      <td>0.029651</td>\n",
       "      <td>0.386707</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.403486</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.397109</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.938034</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.938034</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.850159</td>\n",
       "      <td>0.048024</td>\n",
       "      <td>0.395502</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.937607</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.733145</td>\n",
       "      <td>0.044570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.743947</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.085227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.717942</td>\n",
       "      <td>0.042844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.586703</td>\n",
       "      <td>0.028180</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.442067</td>\n",
       "      <td>0.037310</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.825542</td>\n",
       "      <td>0.015877</td>\n",
       "      <td>0.412681</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.940171</td>\n",
       "      <td>0.940171</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.938034</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.426053</td>\n",
       "      <td>0.070187</td>\n",
       "      <td>0.408131</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.01</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.943376</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.941453</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.723742</td>\n",
       "      <td>0.046659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.713140</td>\n",
       "      <td>0.045847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.790955</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.722342</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.500890</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.390477</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.949786</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.954060</td>\n",
       "      <td>0.952350</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.236832</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>0.387879</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.958761</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.432265</td>\n",
       "      <td>0.030621</td>\n",
       "      <td>0.396281</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.936761</td>\n",
       "      <td>0.047839</td>\n",
       "      <td>0.398479</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>3</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.959615</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.760749</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.743946</td>\n",
       "      <td>0.079678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.725743</td>\n",
       "      <td>0.049975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.749147</td>\n",
       "      <td>0.026364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.634313</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>0.391278</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.955342</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.474673</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.393077</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.964744</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.754728</td>\n",
       "      <td>0.019140</td>\n",
       "      <td>0.406880</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.438449</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>0.404879</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.964744</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.962393</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.928855      0.180074         0.000000        0.000000   \n",
       "1        0.825626      0.042388         0.000000        0.000000   \n",
       "2        0.762808      0.042966         0.000000        0.000000   \n",
       "3        0.775013      0.086321         0.000000        0.000000   \n",
       "4        3.469355      0.019922         0.387106        0.001899   \n",
       "5        5.146816      0.029651         0.386707        0.001020   \n",
       "6        5.403486      0.017657         0.397109        0.001415   \n",
       "7        8.850159      0.048024         0.395502        0.000496   \n",
       "8        0.733145      0.044570         0.000000        0.000000   \n",
       "9        0.743947      0.036539         0.000000        0.000000   \n",
       "10       0.731544      0.085227         0.000000        0.000000   \n",
       "11       0.717942      0.042844         0.000000        0.000000   \n",
       "12       4.586703      0.028180         0.393478        0.001200   \n",
       "13       7.442067      0.037310         0.393478        0.001020   \n",
       "14       7.825542      0.015877         0.412681        0.003930   \n",
       "15      13.426053      0.070187         0.408131        0.001345   \n",
       "16       0.723742      0.046659         0.000000        0.000000   \n",
       "17       0.713140      0.045847         0.000000        0.000000   \n",
       "18       0.790955      0.065200         0.000000        0.000000   \n",
       "19       0.722342      0.043558         0.000000        0.000000   \n",
       "20       3.500890      0.030387         0.390477        0.003721   \n",
       "21       5.236832      0.018674         0.387879        0.001719   \n",
       "22       5.432265      0.030621         0.396281        0.002311   \n",
       "23       8.936761      0.047839         0.398479        0.004801   \n",
       "24       0.760749      0.095745         0.000000        0.000000   \n",
       "25       0.743946      0.079678         0.000000        0.000000   \n",
       "26       0.725743      0.049975         0.000000        0.000000   \n",
       "27       0.749147      0.026364         0.000000        0.000000   \n",
       "28       4.634313      0.028017         0.391278        0.001166   \n",
       "29       7.474673      0.046809         0.393077        0.000002   \n",
       "30       7.754728      0.019140         0.406880        0.005269   \n",
       "31      13.438449      0.077193         0.404879        0.001327   \n",
       "\n",
       "   param_learning_rate param_loss param_max_depth param_max_features  \\\n",
       "0                 0.01   log_loss               3               auto   \n",
       "1                 0.01   log_loss               3               auto   \n",
       "2                 0.01   log_loss               3               auto   \n",
       "3                 0.01   log_loss               3               auto   \n",
       "4                 0.01   log_loss               3               log2   \n",
       "5                 0.01   log_loss               3               log2   \n",
       "6                 0.01   log_loss               3               log2   \n",
       "7                 0.01   log_loss               3               log2   \n",
       "8                 0.01   log_loss               5               auto   \n",
       "9                 0.01   log_loss               5               auto   \n",
       "10                0.01   log_loss               5               auto   \n",
       "11                0.01   log_loss               5               auto   \n",
       "12                0.01   log_loss               5               log2   \n",
       "13                0.01   log_loss               5               log2   \n",
       "14                0.01   log_loss               5               log2   \n",
       "15                0.01   log_loss               5               log2   \n",
       "16                 0.1   log_loss               3               auto   \n",
       "17                 0.1   log_loss               3               auto   \n",
       "18                 0.1   log_loss               3               auto   \n",
       "19                 0.1   log_loss               3               auto   \n",
       "20                 0.1   log_loss               3               log2   \n",
       "21                 0.1   log_loss               3               log2   \n",
       "22                 0.1   log_loss               3               log2   \n",
       "23                 0.1   log_loss               3               log2   \n",
       "24                 0.1   log_loss               5               auto   \n",
       "25                 0.1   log_loss               5               auto   \n",
       "26                 0.1   log_loss               5               auto   \n",
       "27                 0.1   log_loss               5               auto   \n",
       "28                 0.1   log_loss               5               log2   \n",
       "29                 0.1   log_loss               5               log2   \n",
       "30                 0.1   log_loss               5               log2   \n",
       "31                 0.1   log_loss               5               log2   \n",
       "\n",
       "   param_n_estimators param_subsample  \\\n",
       "0                  50             0.5   \n",
       "1                  50               1   \n",
       "2                 100             0.5   \n",
       "3                 100               1   \n",
       "4                  50             0.5   \n",
       "5                  50               1   \n",
       "6                 100             0.5   \n",
       "7                 100               1   \n",
       "8                  50             0.5   \n",
       "9                  50               1   \n",
       "10                100             0.5   \n",
       "11                100               1   \n",
       "12                 50             0.5   \n",
       "13                 50               1   \n",
       "14                100             0.5   \n",
       "15                100               1   \n",
       "16                 50             0.5   \n",
       "17                 50               1   \n",
       "18                100             0.5   \n",
       "19                100               1   \n",
       "20                 50             0.5   \n",
       "21                 50               1   \n",
       "22                100             0.5   \n",
       "23                100               1   \n",
       "24                 50             0.5   \n",
       "25                 50               1   \n",
       "26                100             0.5   \n",
       "27                100               1   \n",
       "28                 50             0.5   \n",
       "29                 50               1   \n",
       "30                100             0.5   \n",
       "31                100               1   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "1   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "2   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "3   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "4   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.935897   \n",
       "5   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.935897   \n",
       "6   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.938034   \n",
       "7   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.939103   \n",
       "8   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "9   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "10  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "11  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...                NaN   \n",
       "12  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.936966   \n",
       "13  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.935897   \n",
       "14  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.940171   \n",
       "15  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...           0.943376   \n",
       "16  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "17  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "18  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "19  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "20  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.948718   \n",
       "21  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.960470   \n",
       "22  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.955128   \n",
       "23  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.960470   \n",
       "24  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "25  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "26  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "27  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...                NaN   \n",
       "28  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.952991   \n",
       "29  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.958333   \n",
       "30  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.959402   \n",
       "31  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.960470   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                 NaN                NaN                NaN   \n",
       "1                 NaN                NaN                NaN   \n",
       "2                 NaN                NaN                NaN   \n",
       "3                 NaN                NaN                NaN   \n",
       "4            0.935897           0.935897           0.935897   \n",
       "5            0.935897           0.935897           0.935897   \n",
       "6            0.936966           0.936966           0.938034   \n",
       "7            0.936966           0.936966           0.939103   \n",
       "8                 NaN                NaN                NaN   \n",
       "9                 NaN                NaN                NaN   \n",
       "10                NaN                NaN                NaN   \n",
       "11                NaN                NaN                NaN   \n",
       "12           0.935897           0.935897           0.935897   \n",
       "13           0.935897           0.935897           0.935897   \n",
       "14           0.940171           0.936966           0.938034   \n",
       "15           0.942308           0.936966           0.942308   \n",
       "16                NaN                NaN                NaN   \n",
       "17                NaN                NaN                NaN   \n",
       "18                NaN                NaN                NaN   \n",
       "19                NaN                NaN                NaN   \n",
       "20           0.952991           0.949786           0.956197   \n",
       "21           0.959402           0.956197           0.961538   \n",
       "22           0.958333           0.958333           0.957265   \n",
       "23           0.960470           0.959402           0.960470   \n",
       "24                NaN                NaN                NaN   \n",
       "25                NaN                NaN                NaN   \n",
       "26                NaN                NaN                NaN   \n",
       "27                NaN                NaN                NaN   \n",
       "28           0.956197           0.955128           0.955128   \n",
       "29           0.961538           0.959402           0.964744   \n",
       "30           0.961538           0.957265           0.962607   \n",
       "31           0.962607           0.962607           0.964744   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0                 NaN              NaN             NaN               17  \n",
       "1                 NaN              NaN             NaN               17  \n",
       "2                 NaN              NaN             NaN               17  \n",
       "3                 NaN              NaN             NaN               17  \n",
       "4            0.934829         0.935684        0.000427               14  \n",
       "5            0.934829         0.935684        0.000427               14  \n",
       "6            0.934829         0.936966        0.001170               12  \n",
       "7            0.935897         0.937607        0.001282               11  \n",
       "8                 NaN              NaN             NaN               17  \n",
       "9                 NaN              NaN             NaN               17  \n",
       "10                NaN              NaN             NaN               17  \n",
       "11                NaN              NaN             NaN               17  \n",
       "12           0.934829         0.935897        0.000676               13  \n",
       "13           0.934829         0.935684        0.000427               14  \n",
       "14           0.936966         0.938462        0.001449               10  \n",
       "15           0.942308         0.941453        0.002281                9  \n",
       "16                NaN              NaN             NaN               17  \n",
       "17                NaN              NaN             NaN               17  \n",
       "18                NaN              NaN             NaN               17  \n",
       "19                NaN              NaN             NaN               17  \n",
       "20           0.954060         0.952350        0.002753                8  \n",
       "21           0.956197         0.958761        0.002200                5  \n",
       "22           0.959402         0.957692        0.001449                6  \n",
       "23           0.957265         0.959615        0.001246                4  \n",
       "24                NaN              NaN             NaN               17  \n",
       "25                NaN              NaN             NaN               17  \n",
       "26                NaN              NaN             NaN               17  \n",
       "27                NaN              NaN             NaN               17  \n",
       "28           0.957265         0.955342        0.001417                7  \n",
       "29           0.959402         0.960684        0.002281                2  \n",
       "30           0.961538         0.960470        0.001911                3  \n",
       "31           0.961538         0.962393        0.001417                1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(grid_search.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a19bcffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.936966</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.937607</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.935684</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'log_loss', 'm...</td>\n",
       "      <td>0.941453</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.952350</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.958761</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.955342</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.962393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score  \\\n",
       "0   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "1   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "2   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "3   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "4   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.935684   \n",
       "5   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.935684   \n",
       "6   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.936966   \n",
       "7   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.937607   \n",
       "8   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "9   {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "10  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "11  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...              NaN   \n",
       "12  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.935897   \n",
       "13  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.935684   \n",
       "14  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.938462   \n",
       "15  {'learning_rate': 0.01, 'loss': 'log_loss', 'm...         0.941453   \n",
       "16  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "17  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "18  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "19  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "20  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.952350   \n",
       "21  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.958761   \n",
       "22  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.957692   \n",
       "23  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.959615   \n",
       "24  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "25  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "26  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "27  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...              NaN   \n",
       "28  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.955342   \n",
       "29  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.960684   \n",
       "30  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.960470   \n",
       "31  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.962393   \n",
       "\n",
       "    rank_test_score  \n",
       "0                17  \n",
       "1                17  \n",
       "2                17  \n",
       "3                17  \n",
       "4                14  \n",
       "5                14  \n",
       "6                12  \n",
       "7                11  \n",
       "8                17  \n",
       "9                17  \n",
       "10               17  \n",
       "11               17  \n",
       "12               13  \n",
       "13               14  \n",
       "14               10  \n",
       "15                9  \n",
       "16               17  \n",
       "17               17  \n",
       "18               17  \n",
       "19               17  \n",
       "20                8  \n",
       "21                5  \n",
       "22                6  \n",
       "23                4  \n",
       "24               17  \n",
       "25               17  \n",
       "26               17  \n",
       "27               17  \n",
       "28                7  \n",
       "29                2  \n",
       "30                3  \n",
       "31                1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[ 'params', 'mean_test_score',  'rank_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30b06555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623931623931623"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa85c32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'log2',\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
