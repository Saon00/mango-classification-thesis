{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45a22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976a8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"TImages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6536a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (244, 244)\n",
    "classes = [\"0\", \"1\", \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c72303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance with data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39319dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in classes:\n",
    "        path = os.path.join(directory, category)\n",
    "        class_num = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))\n",
    "                new_array = cv2.resize(img_array, image_size)\n",
    "\n",
    "                # Generate and store augmented images\n",
    "                augmented_images = []\n",
    "                augmented_images.append(new_array)  # Original image\n",
    "                img_array_aug = new_array.reshape((1,) + new_array.shape)\n",
    "                i = 0\n",
    "                for batch in datagen.flow(img_array_aug, batch_size=1):\n",
    "                    augmented_images.append(batch[0])\n",
    "                    i += 1\n",
    "                    if i >= 5:  # Generate 5 augmented images per input image\n",
    "                        break\n",
    "\n",
    "                for augmented_image in augmented_images:\n",
    "                    image_hsv = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2HSV)\n",
    "                    training_data.append([image_hsv, class_num])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d4e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6011bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5850\n"
     ]
    }
   ],
   "source": [
    "lenofimage = len(training_data)\n",
    "print(lenofimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0e3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for categories, label in training_data:\n",
    "    X.append(categories)\n",
    "    y.append(label)\n",
    "X= np.array(X).reshape(lenofimage,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbf4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8b1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac041ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GB MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265bf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1a7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6554a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {'n_estimators': [100, 150],\n",
    "              'learning_rate': [0.1, 0.3],\n",
    "              'max_depth': [5, 8],\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'loss': ['log_loss'],\n",
    "              'subsample': [0.1, 0.5],\n",
    "             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3b7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch object without cross-validation\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=5, verbose=2,error_score='raise')  # Set cv=None for no cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "926c776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  27.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  27.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  27.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  27.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 6.3min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 6.2min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 6.2min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 6.2min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.1min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 4.9min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.2min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 4.9min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 4.8min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   9.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   9.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   9.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   9.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   9.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=  18.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=  14.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=  10.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=  10.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   6.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   6.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   7.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   7.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   6.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  14.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  13.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  14.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  14.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  19.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  56.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  49.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.7min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.3min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.3min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  59.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  58.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  57.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.4s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.2s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.3s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.5s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.9s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.7s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.8s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.6s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.1s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  17.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.0s\n",
      "[CV] END learning_rate=0.1, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.4s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  26.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 2.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 2.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 2.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 2.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 2.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  39.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  38.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  39.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  38.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  38.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 3.2min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 3.2min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 3.2min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 3.2min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 3.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   3.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   3.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   3.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   3.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.1; total time=   3.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   7.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   7.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   7.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   7.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=100, subsample=0.5; total time=   7.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   4.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   4.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   4.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   4.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.1; total time=   4.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  11.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  10.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  11.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  10.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=5, max_features=log2, n_estimators=150, subsample=0.5; total time=  11.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  38.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  37.1s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  38.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  38.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.1; total time=  37.7s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=100, subsample=0.5; total time= 3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  59.4s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  56.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time=  59.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.3min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.1; total time= 1.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.2min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.0min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=sqrt, n_estimators=150, subsample=0.5; total time= 5.1min\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.1; total time=   4.3s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  12.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.4s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  12.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.5s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=100, subsample=0.5; total time=  11.6s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.8s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   6.0s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.1; total time=   5.9s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  17.1s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.3s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  17.2s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.3s\n",
      "[CV] END learning_rate=0.3, loss=log_loss, max_depth=8, max_features=log2, n_estimators=150, subsample=0.5; total time=  16.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, error_score=&#x27;raise&#x27;, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.3], &#x27;loss&#x27;: [&#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 8], &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150], &#x27;subsample&#x27;: [0.1, 0.5]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, error_score=&#x27;raise&#x27;, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.3], &#x27;loss&#x27;: [&#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 8], &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150], &#x27;subsample&#x27;: [0.1, 0.5]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise', estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.3], 'loss': ['log_loss'],\n",
       "                         'max_depth': [5, 8], 'max_features': ['sqrt', 'log2'],\n",
       "                         'n_estimators': [100, 150], 'subsample': [0.1, 0.5]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the GridSearch object to your data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f655292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Gaussian Naive Bayes Parameters: {'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100, 'subsample': 0.5}\n",
      "Accuracy: 95.04%\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimator and evaluate it\n",
    "best_nb = grid_search.best_estimator_\n",
    "y_test_pred = best_nb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best Gaussian Naive Bayes Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b819f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work progressing...........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e85d5980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 26.79219799, 341.68242688,  97.90530262, 297.85650697,\n",
       "          8.43672547,  12.14344125,   6.29513726,  14.62389903,\n",
       "         57.13575745, 205.39720654,  59.04720416, 300.9145391 ,\n",
       "          4.09094062,  11.09113092,   5.47524753,  16.1162941 ,\n",
       "         26.19519186, 126.75956292,  38.56109529, 189.29962201,\n",
       "          3.32509069,   7.54397707,   4.30650554,  10.65969262,\n",
       "         37.77285595, 202.37697239,  63.57156696, 306.19361453,\n",
       "          4.18208065,  11.65900888,   5.55029669,  16.31768837]),\n",
       " 'std_fit_time': array([1.52260807e-01, 6.13758276e+01, 3.49388464e-01, 1.01104252e+01,\n",
       "        1.68185839e-01, 2.83225796e+00, 1.98809431e-01, 1.71543960e+00,\n",
       "        4.63701765e+00, 9.28912484e+00, 1.14721890e+00, 7.84657994e-01,\n",
       "        1.81302488e-02, 1.37434722e-01, 8.64966087e-02, 3.47608725e-01,\n",
       "        5.27705047e-02, 2.86272502e-01, 1.19307373e-01, 4.86796005e-01,\n",
       "        3.75037236e-02, 2.86449067e-02, 7.83057234e-02, 3.87814367e-02,\n",
       "        6.40844975e-01, 9.05723129e-01, 7.30740738e+00, 2.80608738e+00,\n",
       "        8.57829836e-02, 5.84758260e-01, 6.49485465e-02, 3.98505910e-01]),\n",
       " 'mean_score_time': array([0.39980025, 1.7400691 , 1.66106763, 0.95042024, 1.21886668,\n",
       "        0.85024943, 0.72522926, 0.86590047, 1.05993361, 0.40940485,\n",
       "        0.42189984, 0.42189651, 0.41877356, 0.41876926, 0.4375236 ,\n",
       "        0.43439317, 0.39376311, 0.39376445, 0.40939007, 0.4031374 ,\n",
       "        0.40627346, 0.39688678, 0.41876974, 0.4125124 , 0.40938621,\n",
       "        0.40730996, 0.60981331, 0.42440867, 0.42220774, 0.41600742,\n",
       "        0.44620767, 0.44020767]),\n",
       " 'std_score_time': array([1.93791392e-03, 7.81637956e-02, 4.29576935e-01, 1.81731870e-01,\n",
       "        5.23259353e-02, 1.81982295e-01, 8.01250726e-02, 1.86269733e-01,\n",
       "        2.05216438e-01, 6.25116899e-03, 1.78416128e-07, 2.59164395e-06,\n",
       "        6.25248165e-03, 6.25021458e-03, 4.67932527e-06, 6.24859467e-03,\n",
       "        6.24570859e-03, 6.25004769e-03, 6.25009537e-03, 6.25011921e-03,\n",
       "        9.48893964e-07, 7.65506425e-03, 6.24711690e-03, 7.65485012e-03,\n",
       "        6.24964250e-03, 2.35231241e-03, 3.51142881e-01, 2.49896797e-03,\n",
       "        4.83311672e-03, 2.28045746e-03, 4.35464091e-03, 2.78562985e-03]),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_loss': masked_array(data=['log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 100, 150, 150, 100, 100, 150, 150, 100, 100, 150,\n",
       "                    150, 100, 100, 150, 150, 100, 100, 150, 150, 100, 100,\n",
       "                    150, 150, 100, 100, 150, 150, 100, 100, 150, 150],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1,\n",
       "                    0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5,\n",
       "                    0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5, 0.1, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.1},\n",
       "  {'learning_rate': 0.3,\n",
       "   'loss': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'n_estimators': 150,\n",
       "   'subsample': 0.5}],\n",
       " 'split0_test_score': array([0.86004274, 0.95726496, 0.91773504, 0.95940171, 0.23931624,\n",
       "        0.95833333, 0.79700855, 0.96153846, 0.18055556, 0.9465812 ,\n",
       "        0.86324786, 0.95405983, 0.35683761, 0.95833333, 0.56303419,\n",
       "        0.95940171, 0.08226496, 0.88995726, 0.06196581, 0.88888889,\n",
       "        0.37713675, 0.90811966, 0.08653846, 0.93055556, 0.68376068,\n",
       "        0.88247863, 0.86538462, 0.90277778, 0.56837607, 0.86324786,\n",
       "        0.26495726, 0.49893162]),\n",
       " 'split1_test_score': array([0.85042735, 0.95512821, 0.03098291, 0.95619658, 0.70833333,\n",
       "        0.96260684, 0.18589744, 0.96047009, 0.89636752, 0.96153846,\n",
       "        0.18055556, 0.95833333, 0.18910256, 0.95940171, 0.16239316,\n",
       "        0.95940171, 0.91452991, 0.91132479, 0.80662393, 0.89423077,\n",
       "        0.10897436, 0.92307692, 0.68162393, 0.36431624, 0.16346154,\n",
       "        0.91346154, 0.07264957, 0.90064103, 0.31196581, 0.8974359 ,\n",
       "        0.72008547, 0.61111111]),\n",
       " 'split2_test_score': array([0.03418803, 0.94871795, 0.15811966, 0.94444444, 0.89636752,\n",
       "        0.95726496, 0.10897436, 0.95512821, 0.04273504, 0.95192308,\n",
       "        0.86645299, 0.95726496, 0.40491453, 0.95833333, 0.42628205,\n",
       "        0.95726496, 0.17200855, 0.8974359 , 0.02777778, 0.88675214,\n",
       "        0.46260684, 0.88995726, 0.40384615, 0.88034188, 0.02564103,\n",
       "        0.87713675, 0.10576923, 0.86004274, 0.40491453, 0.91559829,\n",
       "        0.29594017, 0.33226496]),\n",
       " 'split3_test_score': array([0.05448718, 0.95619658, 0.03418803, 0.95619658, 0.21047009,\n",
       "        0.96153846, 0.30982906, 0.95940171, 0.85897436, 0.95405983,\n",
       "        0.20833333, 0.95726496, 0.15491453, 0.96047009, 0.1784188 ,\n",
       "        0.96153846, 0.13782051, 0.88141026, 0.05662393, 0.88675214,\n",
       "        0.46474359, 0.91666667, 0.02884615, 0.93589744, 0.81944444,\n",
       "        0.88995726, 0.23931624, 0.88888889, 0.69551282, 0.95192308,\n",
       "        0.68376068, 0.44444444]),\n",
       " 'split4_test_score': array([0.88034188, 0.96047009, 0.82264957, 0.96367521, 0.76175214,\n",
       "        0.96260684, 0.35683761, 0.96260684, 0.86217949, 0.96047009,\n",
       "        0.04700855, 0.96153846, 0.28205128, 0.96474359, 0.31837607,\n",
       "        0.96153846, 0.74145299, 0.90598291, 0.0491453 , 0.89102564,\n",
       "        0.62713675, 0.76816239, 0.30982906, 0.65277778, 0.34401709,\n",
       "        0.89850427, 0.66666667, 0.90384615, 0.79807692, 0.89957265,\n",
       "        0.53739316, 0.03952991]),\n",
       " 'mean_test_score': array([0.53589744, 0.95555556, 0.39273504, 0.95598291, 0.56324786,\n",
       "        0.96047009, 0.3517094 , 0.95982906, 0.56816239, 0.95491453,\n",
       "        0.43311966, 0.95769231, 0.2775641 , 0.96025641, 0.32970085,\n",
       "        0.95982906, 0.40961538, 0.89722222, 0.20042735, 0.88952991,\n",
       "        0.40811966, 0.88119658, 0.30213675, 0.75277778, 0.40726496,\n",
       "        0.89230769, 0.38995726, 0.89123932, 0.55576923, 0.90555556,\n",
       "        0.50042735, 0.38525641]),\n",
       " 'std_test_score': array([0.40152444, 0.00385801, 0.39367917, 0.00638885, 0.28312764,\n",
       "        0.00224104, 0.23936813, 0.00258185, 0.37551277, 0.00554733,\n",
       "        0.35670362, 0.0023985 , 0.0952795 , 0.00237939, 0.15151155,\n",
       "        0.001599  , 0.34714338, 0.01075616, 0.30332143, 0.00283472,\n",
       "        0.17005379, 0.05760281, 0.23477004, 0.2201421 , 0.30180094,\n",
       "        0.01279199, 0.31835169, 0.01648764, 0.17921912, 0.02878357,\n",
       "        0.18999392, 0.19481813]),\n",
       " 'rank_test_score': array([19,  7, 25,  6, 17,  1, 28,  3, 16,  8, 21,  5, 31,  2, 29,  3, 22,\n",
       "        10, 32, 13, 23, 14, 30, 15, 24, 11, 26, 12, 18,  9, 20, 27])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e37c36c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.792198</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>1.937914e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.860043</td>\n",
       "      <td>0.850427</td>\n",
       "      <td>0.034188</td>\n",
       "      <td>0.054487</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>341.682427</td>\n",
       "      <td>61.375828</td>\n",
       "      <td>1.740069</td>\n",
       "      <td>7.816380e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.905303</td>\n",
       "      <td>0.349388</td>\n",
       "      <td>1.661068</td>\n",
       "      <td>4.295769e-01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.917735</td>\n",
       "      <td>0.030983</td>\n",
       "      <td>0.158120</td>\n",
       "      <td>0.034188</td>\n",
       "      <td>0.822650</td>\n",
       "      <td>0.392735</td>\n",
       "      <td>0.393679</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>297.856507</td>\n",
       "      <td>10.110425</td>\n",
       "      <td>0.950420</td>\n",
       "      <td>1.817319e-01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.963675</td>\n",
       "      <td>0.955983</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.436725</td>\n",
       "      <td>0.168186</td>\n",
       "      <td>1.218867</td>\n",
       "      <td>5.232594e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.896368</td>\n",
       "      <td>0.210470</td>\n",
       "      <td>0.761752</td>\n",
       "      <td>0.563248</td>\n",
       "      <td>0.283128</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.143441</td>\n",
       "      <td>2.832258</td>\n",
       "      <td>0.850249</td>\n",
       "      <td>1.819823e-01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.295137</td>\n",
       "      <td>0.198809</td>\n",
       "      <td>0.725229</td>\n",
       "      <td>8.012507e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.185897</td>\n",
       "      <td>0.108974</td>\n",
       "      <td>0.309829</td>\n",
       "      <td>0.356838</td>\n",
       "      <td>0.351709</td>\n",
       "      <td>0.239368</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.623899</td>\n",
       "      <td>1.715440</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>1.862697e-01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.962607</td>\n",
       "      <td>0.959829</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57.135757</td>\n",
       "      <td>4.637018</td>\n",
       "      <td>1.059934</td>\n",
       "      <td>2.052164e-01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.896368</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.862179</td>\n",
       "      <td>0.568162</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>205.397207</td>\n",
       "      <td>9.289125</td>\n",
       "      <td>0.409405</td>\n",
       "      <td>6.251169e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.954060</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.954915</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59.047204</td>\n",
       "      <td>1.147219</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>1.784161e-07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.866453</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>0.433120</td>\n",
       "      <td>0.356704</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>300.914539</td>\n",
       "      <td>0.784658</td>\n",
       "      <td>0.421897</td>\n",
       "      <td>2.591644e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.954060</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.090941</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.418774</td>\n",
       "      <td>6.252482e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.356838</td>\n",
       "      <td>0.189103</td>\n",
       "      <td>0.404915</td>\n",
       "      <td>0.154915</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.277564</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.091131</td>\n",
       "      <td>0.137435</td>\n",
       "      <td>0.418769</td>\n",
       "      <td>6.250215e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.964744</td>\n",
       "      <td>0.960256</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.475248</td>\n",
       "      <td>0.086497</td>\n",
       "      <td>0.437524</td>\n",
       "      <td>4.679325e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.563034</td>\n",
       "      <td>0.162393</td>\n",
       "      <td>0.426282</td>\n",
       "      <td>0.178419</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.151512</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.116294</td>\n",
       "      <td>0.347609</td>\n",
       "      <td>0.434393</td>\n",
       "      <td>6.248595e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.959829</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26.195192</td>\n",
       "      <td>0.052771</td>\n",
       "      <td>0.393763</td>\n",
       "      <td>6.245709e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.082265</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.172009</td>\n",
       "      <td>0.137821</td>\n",
       "      <td>0.741453</td>\n",
       "      <td>0.409615</td>\n",
       "      <td>0.347143</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>126.759563</td>\n",
       "      <td>0.286273</td>\n",
       "      <td>0.393764</td>\n",
       "      <td>6.250048e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.889957</td>\n",
       "      <td>0.911325</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.881410</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38.561095</td>\n",
       "      <td>0.119307</td>\n",
       "      <td>0.409390</td>\n",
       "      <td>6.250095e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.049145</td>\n",
       "      <td>0.200427</td>\n",
       "      <td>0.303321</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>189.299622</td>\n",
       "      <td>0.486796</td>\n",
       "      <td>0.403137</td>\n",
       "      <td>6.250119e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.886752</td>\n",
       "      <td>0.886752</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.889530</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.325091</td>\n",
       "      <td>0.037504</td>\n",
       "      <td>0.406273</td>\n",
       "      <td>9.488940e-07</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.377137</td>\n",
       "      <td>0.108974</td>\n",
       "      <td>0.462607</td>\n",
       "      <td>0.464744</td>\n",
       "      <td>0.627137</td>\n",
       "      <td>0.408120</td>\n",
       "      <td>0.170054</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.543977</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.396887</td>\n",
       "      <td>7.655064e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.889957</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.768162</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.057603</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.306506</td>\n",
       "      <td>0.078306</td>\n",
       "      <td>0.418770</td>\n",
       "      <td>6.247117e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.681624</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.309829</td>\n",
       "      <td>0.302137</td>\n",
       "      <td>0.234770</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.659693</td>\n",
       "      <td>0.038781</td>\n",
       "      <td>0.412512</td>\n",
       "      <td>7.654850e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.364316</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.220142</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37.772856</td>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.409386</td>\n",
       "      <td>6.249643e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.344017</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>0.301801</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>202.376972</td>\n",
       "      <td>0.905723</td>\n",
       "      <td>0.407310</td>\n",
       "      <td>2.352312e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.877137</td>\n",
       "      <td>0.889957</td>\n",
       "      <td>0.898504</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.012792</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>63.571567</td>\n",
       "      <td>7.307407</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>3.511429e-01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>0.105769</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.389957</td>\n",
       "      <td>0.318352</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>306.193615</td>\n",
       "      <td>2.806087</td>\n",
       "      <td>0.424409</td>\n",
       "      <td>2.498968e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>0.860043</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.182081</td>\n",
       "      <td>0.085783</td>\n",
       "      <td>0.422208</td>\n",
       "      <td>4.833117e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.568376</td>\n",
       "      <td>0.311966</td>\n",
       "      <td>0.404915</td>\n",
       "      <td>0.695513</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.555769</td>\n",
       "      <td>0.179219</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.659009</td>\n",
       "      <td>0.584758</td>\n",
       "      <td>0.416007</td>\n",
       "      <td>2.280457e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.915598</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.899573</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.028784</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.550297</td>\n",
       "      <td>0.064949</td>\n",
       "      <td>0.446208</td>\n",
       "      <td>4.354641e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.720085</td>\n",
       "      <td>0.295940</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>0.537393</td>\n",
       "      <td>0.500427</td>\n",
       "      <td>0.189994</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>16.317688</td>\n",
       "      <td>0.398506</td>\n",
       "      <td>0.440208</td>\n",
       "      <td>2.785630e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.498932</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.332265</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.039530</td>\n",
       "      <td>0.385256</td>\n",
       "      <td>0.194818</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       26.792198      0.152261         0.399800    1.937914e-03   \n",
       "1      341.682427     61.375828         1.740069    7.816380e-02   \n",
       "2       97.905303      0.349388         1.661068    4.295769e-01   \n",
       "3      297.856507     10.110425         0.950420    1.817319e-01   \n",
       "4        8.436725      0.168186         1.218867    5.232594e-02   \n",
       "5       12.143441      2.832258         0.850249    1.819823e-01   \n",
       "6        6.295137      0.198809         0.725229    8.012507e-02   \n",
       "7       14.623899      1.715440         0.865900    1.862697e-01   \n",
       "8       57.135757      4.637018         1.059934    2.052164e-01   \n",
       "9      205.397207      9.289125         0.409405    6.251169e-03   \n",
       "10      59.047204      1.147219         0.421900    1.784161e-07   \n",
       "11     300.914539      0.784658         0.421897    2.591644e-06   \n",
       "12       4.090941      0.018130         0.418774    6.252482e-03   \n",
       "13      11.091131      0.137435         0.418769    6.250215e-03   \n",
       "14       5.475248      0.086497         0.437524    4.679325e-06   \n",
       "15      16.116294      0.347609         0.434393    6.248595e-03   \n",
       "16      26.195192      0.052771         0.393763    6.245709e-03   \n",
       "17     126.759563      0.286273         0.393764    6.250048e-03   \n",
       "18      38.561095      0.119307         0.409390    6.250095e-03   \n",
       "19     189.299622      0.486796         0.403137    6.250119e-03   \n",
       "20       3.325091      0.037504         0.406273    9.488940e-07   \n",
       "21       7.543977      0.028645         0.396887    7.655064e-03   \n",
       "22       4.306506      0.078306         0.418770    6.247117e-03   \n",
       "23      10.659693      0.038781         0.412512    7.654850e-03   \n",
       "24      37.772856      0.640845         0.409386    6.249643e-03   \n",
       "25     202.376972      0.905723         0.407310    2.352312e-03   \n",
       "26      63.571567      7.307407         0.609813    3.511429e-01   \n",
       "27     306.193615      2.806087         0.424409    2.498968e-03   \n",
       "28       4.182081      0.085783         0.422208    4.833117e-03   \n",
       "29      11.659009      0.584758         0.416007    2.280457e-03   \n",
       "30       5.550297      0.064949         0.446208    4.354641e-03   \n",
       "31      16.317688      0.398506         0.440208    2.785630e-03   \n",
       "\n",
       "   param_learning_rate param_loss param_max_depth param_max_features  \\\n",
       "0                  0.1   log_loss               5               sqrt   \n",
       "1                  0.1   log_loss               5               sqrt   \n",
       "2                  0.1   log_loss               5               sqrt   \n",
       "3                  0.1   log_loss               5               sqrt   \n",
       "4                  0.1   log_loss               5               log2   \n",
       "5                  0.1   log_loss               5               log2   \n",
       "6                  0.1   log_loss               5               log2   \n",
       "7                  0.1   log_loss               5               log2   \n",
       "8                  0.1   log_loss               8               sqrt   \n",
       "9                  0.1   log_loss               8               sqrt   \n",
       "10                 0.1   log_loss               8               sqrt   \n",
       "11                 0.1   log_loss               8               sqrt   \n",
       "12                 0.1   log_loss               8               log2   \n",
       "13                 0.1   log_loss               8               log2   \n",
       "14                 0.1   log_loss               8               log2   \n",
       "15                 0.1   log_loss               8               log2   \n",
       "16                 0.3   log_loss               5               sqrt   \n",
       "17                 0.3   log_loss               5               sqrt   \n",
       "18                 0.3   log_loss               5               sqrt   \n",
       "19                 0.3   log_loss               5               sqrt   \n",
       "20                 0.3   log_loss               5               log2   \n",
       "21                 0.3   log_loss               5               log2   \n",
       "22                 0.3   log_loss               5               log2   \n",
       "23                 0.3   log_loss               5               log2   \n",
       "24                 0.3   log_loss               8               sqrt   \n",
       "25                 0.3   log_loss               8               sqrt   \n",
       "26                 0.3   log_loss               8               sqrt   \n",
       "27                 0.3   log_loss               8               sqrt   \n",
       "28                 0.3   log_loss               8               log2   \n",
       "29                 0.3   log_loss               8               log2   \n",
       "30                 0.3   log_loss               8               log2   \n",
       "31                 0.3   log_loss               8               log2   \n",
       "\n",
       "   param_n_estimators param_subsample  \\\n",
       "0                 100             0.1   \n",
       "1                 100             0.5   \n",
       "2                 150             0.1   \n",
       "3                 150             0.5   \n",
       "4                 100             0.1   \n",
       "5                 100             0.5   \n",
       "6                 150             0.1   \n",
       "7                 150             0.5   \n",
       "8                 100             0.1   \n",
       "9                 100             0.5   \n",
       "10                150             0.1   \n",
       "11                150             0.5   \n",
       "12                100             0.1   \n",
       "13                100             0.5   \n",
       "14                150             0.1   \n",
       "15                150             0.5   \n",
       "16                100             0.1   \n",
       "17                100             0.5   \n",
       "18                150             0.1   \n",
       "19                150             0.5   \n",
       "20                100             0.1   \n",
       "21                100             0.5   \n",
       "22                150             0.1   \n",
       "23                150             0.5   \n",
       "24                100             0.1   \n",
       "25                100             0.5   \n",
       "26                150             0.1   \n",
       "27                150             0.5   \n",
       "28                100             0.1   \n",
       "29                100             0.5   \n",
       "30                150             0.1   \n",
       "31                150             0.5   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.860043   \n",
       "1   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.957265   \n",
       "2   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.917735   \n",
       "3   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.959402   \n",
       "4   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.239316   \n",
       "5   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.958333   \n",
       "6   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.797009   \n",
       "7   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.961538   \n",
       "8   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.180556   \n",
       "9   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.946581   \n",
       "10  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.863248   \n",
       "11  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.954060   \n",
       "12  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.356838   \n",
       "13  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.958333   \n",
       "14  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.563034   \n",
       "15  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...           0.959402   \n",
       "16  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.082265   \n",
       "17  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.889957   \n",
       "18  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.061966   \n",
       "19  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.888889   \n",
       "20  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.377137   \n",
       "21  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.908120   \n",
       "22  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.086538   \n",
       "23  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.930556   \n",
       "24  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.683761   \n",
       "25  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.882479   \n",
       "26  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.865385   \n",
       "27  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.902778   \n",
       "28  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.568376   \n",
       "29  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.863248   \n",
       "30  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.264957   \n",
       "31  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...           0.498932   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.850427           0.034188           0.054487   \n",
       "1            0.955128           0.948718           0.956197   \n",
       "2            0.030983           0.158120           0.034188   \n",
       "3            0.956197           0.944444           0.956197   \n",
       "4            0.708333           0.896368           0.210470   \n",
       "5            0.962607           0.957265           0.961538   \n",
       "6            0.185897           0.108974           0.309829   \n",
       "7            0.960470           0.955128           0.959402   \n",
       "8            0.896368           0.042735           0.858974   \n",
       "9            0.961538           0.951923           0.954060   \n",
       "10           0.180556           0.866453           0.208333   \n",
       "11           0.958333           0.957265           0.957265   \n",
       "12           0.189103           0.404915           0.154915   \n",
       "13           0.959402           0.958333           0.960470   \n",
       "14           0.162393           0.426282           0.178419   \n",
       "15           0.959402           0.957265           0.961538   \n",
       "16           0.914530           0.172009           0.137821   \n",
       "17           0.911325           0.897436           0.881410   \n",
       "18           0.806624           0.027778           0.056624   \n",
       "19           0.894231           0.886752           0.886752   \n",
       "20           0.108974           0.462607           0.464744   \n",
       "21           0.923077           0.889957           0.916667   \n",
       "22           0.681624           0.403846           0.028846   \n",
       "23           0.364316           0.880342           0.935897   \n",
       "24           0.163462           0.025641           0.819444   \n",
       "25           0.913462           0.877137           0.889957   \n",
       "26           0.072650           0.105769           0.239316   \n",
       "27           0.900641           0.860043           0.888889   \n",
       "28           0.311966           0.404915           0.695513   \n",
       "29           0.897436           0.915598           0.951923   \n",
       "30           0.720085           0.295940           0.683761   \n",
       "31           0.611111           0.332265           0.444444   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.880342         0.535897        0.401524               19  \n",
       "1            0.960470         0.955556        0.003858                7  \n",
       "2            0.822650         0.392735        0.393679               25  \n",
       "3            0.963675         0.955983        0.006389                6  \n",
       "4            0.761752         0.563248        0.283128               17  \n",
       "5            0.962607         0.960470        0.002241                1  \n",
       "6            0.356838         0.351709        0.239368               28  \n",
       "7            0.962607         0.959829        0.002582                3  \n",
       "8            0.862179         0.568162        0.375513               16  \n",
       "9            0.960470         0.954915        0.005547                8  \n",
       "10           0.047009         0.433120        0.356704               21  \n",
       "11           0.961538         0.957692        0.002398                5  \n",
       "12           0.282051         0.277564        0.095280               31  \n",
       "13           0.964744         0.960256        0.002379                2  \n",
       "14           0.318376         0.329701        0.151512               29  \n",
       "15           0.961538         0.959829        0.001599                3  \n",
       "16           0.741453         0.409615        0.347143               22  \n",
       "17           0.905983         0.897222        0.010756               10  \n",
       "18           0.049145         0.200427        0.303321               32  \n",
       "19           0.891026         0.889530        0.002835               13  \n",
       "20           0.627137         0.408120        0.170054               23  \n",
       "21           0.768162         0.881197        0.057603               14  \n",
       "22           0.309829         0.302137        0.234770               30  \n",
       "23           0.652778         0.752778        0.220142               15  \n",
       "24           0.344017         0.407265        0.301801               24  \n",
       "25           0.898504         0.892308        0.012792               11  \n",
       "26           0.666667         0.389957        0.318352               26  \n",
       "27           0.903846         0.891239        0.016488               12  \n",
       "28           0.798077         0.555769        0.179219               18  \n",
       "29           0.899573         0.905556        0.028784                9  \n",
       "30           0.537393         0.500427        0.189994               20  \n",
       "31           0.039530         0.385256        0.194818               27  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(grid_search.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a19bcffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.392735</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.955983</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.563248</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.351709</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959829</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.568162</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.954915</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.433120</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.277564</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.960256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.959829</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.409615</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.200427</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.889530</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.408120</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.302137</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.752778</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.407265</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.389957</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.555769</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.500427</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'learning_rate': 0.3, 'loss': 'log_loss', 'ma...</td>\n",
       "      <td>0.385256</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score  \\\n",
       "0   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.535897   \n",
       "1   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.955556   \n",
       "2   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.392735   \n",
       "3   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.955983   \n",
       "4   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.563248   \n",
       "5   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.960470   \n",
       "6   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.351709   \n",
       "7   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.959829   \n",
       "8   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.568162   \n",
       "9   {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.954915   \n",
       "10  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.433120   \n",
       "11  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.957692   \n",
       "12  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.277564   \n",
       "13  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.960256   \n",
       "14  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.329701   \n",
       "15  {'learning_rate': 0.1, 'loss': 'log_loss', 'ma...         0.959829   \n",
       "16  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.409615   \n",
       "17  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.897222   \n",
       "18  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.200427   \n",
       "19  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.889530   \n",
       "20  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.408120   \n",
       "21  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.881197   \n",
       "22  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.302137   \n",
       "23  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.752778   \n",
       "24  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.407265   \n",
       "25  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.892308   \n",
       "26  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.389957   \n",
       "27  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.891239   \n",
       "28  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.555769   \n",
       "29  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.905556   \n",
       "30  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.500427   \n",
       "31  {'learning_rate': 0.3, 'loss': 'log_loss', 'ma...         0.385256   \n",
       "\n",
       "    rank_test_score  \n",
       "0                19  \n",
       "1                 7  \n",
       "2                25  \n",
       "3                 6  \n",
       "4                17  \n",
       "5                 1  \n",
       "6                28  \n",
       "7                 3  \n",
       "8                16  \n",
       "9                 8  \n",
       "10               21  \n",
       "11                5  \n",
       "12               31  \n",
       "13                2  \n",
       "14               29  \n",
       "15                3  \n",
       "16               22  \n",
       "17               10  \n",
       "18               32  \n",
       "19               13  \n",
       "20               23  \n",
       "21               14  \n",
       "22               30  \n",
       "23               15  \n",
       "24               24  \n",
       "25               11  \n",
       "26               26  \n",
       "27               12  \n",
       "28               18  \n",
       "29                9  \n",
       "30               20  \n",
       "31               27  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[ 'params', 'mean_test_score',  'rank_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30b06555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9604700854700855"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa85c32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'log2',\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f74b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
